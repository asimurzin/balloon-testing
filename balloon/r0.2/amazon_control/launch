#!/usr/bin/env python

#--------------------------------------------------------------------------------------
## Copyright 2010 Alexey Petrov
##
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##     http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.


#--------------------------------------------------------------------------------------
"""
Callback script which is called automatically on delivering task to cloud
"""

#--------------------------------------------------------------------------------------
import balloon.common as common
from balloon.common import print_d, print_e, sh_command

import balloon.amazon as amazon

import os, os.path, uuid


#--------------------------------------------------------------------------------------
# Defining utility command-line interface

an_usage_description = "%prog --bucket-name=33f89d9d-5417-49c1-80c9-787e74cc7154 --data-name='task_data.tgz' --working-dir='/tmp/tmpt7lweb'"
an_usage_description += common.add_usage_description()
an_usage_description += amazon.add_usage_description()

from optparse import IndentedHelpFormatter
a_help_formatter = IndentedHelpFormatter( width = 127 )

from optparse import OptionParser
a_option_parser = OptionParser( usage = an_usage_description, version="%prog 0.1", formatter = a_help_formatter )

# Definition of the command line arguments
a_option_parser.add_option( "--bucket-name",
                            metavar = "< name of task dedicated Amazon S3 bucket >",
                            action = "store",
                            dest = "bucket_name" )

a_option_parser.add_option( "--data-name",
                            metavar = "< container file object representing initial task data >",
                            action = "store",
                            dest = "data_name" )
    
a_option_parser.add_option( "--working-dir",
                            metavar = "< task working dir >",
                            action = "store",
                            dest = "working_dir" )
    
common.add_parser_options( a_option_parser )

amazon.add_parser_options( a_option_parser )
     

#--------------------------------------------------------------------------------------
# Fetching the cloud provider security cridentials
an_options, an_args = a_option_parser.parse_args()

a_bucket_name = an_options.bucket_name
a_data_name = an_options.data_name
a_working_dir = an_options.working_dir

AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY = amazon.extract_options( an_options )


print_d( "\n---------------------------------------------------------------------------\n" )
# Getting access to corresponding cloudfile container
a_data_loading_time = Timer()

import boto
a_s3_conn = boto.connect_s3( AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY )
print_d( "a_s3_conn = %r\n" % a_s3_conn )

a_s3_bucket = a_s3_conn.get_bucket( a_bucket_name )
print_d( "a_s3_bucket = %s\n" % a_s3_bucket.name )

from boto.s3.key import Key
a_s3_bucket_key = a_s3_bucket.get_key( a_data_name )
print_d( "a_s3_bucket_key = %s\n" % a_s3_bucket_key.name )


#--------------------------------------------------------------------------------------
# Defining a simple automatic context dependatent task:
#  - downloading corresponding 'data' archive from cloudfiles
#  - extracting its context into the working directory
#  - uploading back all the data items as separate objects into initial cloudfile container

import os.path
a_data_archive = os.path.join( a_working_dir, a_data_name )
a_s3_bucket_key.get_contents_from_filename( a_data_archive )

print_d( "a_data_loading_time = %s, sec\n" % a_data_loading_time )


#---------------------------------------------------------------------------
# Preparing file information to upload

sh_command( 'cd %s && tar -xzf %s' % ( a_working_dir, a_data_name ) )

a_data_dir = a_working_dir
a_files = os.listdir( a_data_dir )
a_files.sort()


#--------------------------------------------------------------------------------------
# Create an Amazon Simple Queue Service (SQS) queue to support notification
# between cloud task execution process and remote client

import boto
a_sqs_conn = boto.connect_sqs( AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY )
a_queue_name = common.generate_initial_queue_name( a_container_name )


#--------------------------------------------------------------------------------------
for a_file in a_files :
    a_file_path = os.path.join( a_data_dir, a_file )
    if os.path.isdir( a_file_path ) :
        continue
    a_file_object = a_cloudfiles_container.create_object( a_file )
    a_file_object.load_from_filename( a_file_path )
    print_d( '%s\n' % a_file_path )

    # Define ( queue, message ) linked list
    a_sqs_queue = a_sqs_conn.create_queue( a_queue_name )
    print_d( '%s\n' % a_queue_name )

    a_queue_suffix = str( uuid.uuid4() )
    a_queue_name = common.generate_queue_name( a_container_name, a_queue_suffix )

    a_message_body = common.generate_message_body( a_file, a_queue_suffix )
    a_status = common.push_message( a_sqs_queue, a_message_body )

    pass


#--------------------------------------------------------------------------------------
# The terminal node for the ( queue, message ) linked list
a_sqs_queue = a_sqs_conn.create_queue( a_queue_name )
a_message_body = common.generate_final_message_body( a_container_name )
a_status = common.push_message( a_sqs_queue, a_message_body )

# Publishing "final" object marker in the 'cloudfiles'
a_file_object = a_cloudfiles_container.create_object( a_container_name )
a_file_object.write( '' )


#--------------------------------------------------------------------------------------
print_d( 'OK\n' )


#--------------------------------------------------------------------------------------
